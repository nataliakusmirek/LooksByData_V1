{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71c76d-d161-4db3-bd76-393f3063775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Article content extracted and saved to articletext/Skip to main content_content.txt\n",
      "INFO:root:Article content extracted and saved to articletext/Skip to main content_content.txt\n",
      "INFO:root:Article content extracted and saved to articletext/Skip to main content_content.txt\n",
      "INFO:root:Article content extracted and saved to articletext/Skip to main content_content.txt\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/trends\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/trends\n",
      "ERROR:root:Error occurred while fetching HTML from http://www.aboutads.info: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/models\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/models\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/street-style\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/street-style\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/celebrity-style\n",
      "INFO:root:Scraped page: https://www.vogue.com/fashion/celebrity-style\n",
      "ERROR:root:Error occurred while fetching HTML from http://www.aboutads.info: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "ERROR:root:Error occurred while fetching HTML from http://www.aboutads.info: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "WARNING:root:Received 503 status code. Retrying after delay.\n",
      "ERROR:root:Error occurred while fetching HTML from http://www.aboutads.info: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from queue import PriorityQueue\n",
    "import threading\n",
    "import logging\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import csv\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"s3.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Value to store URLs that have already been seen\n",
    "seen_urls = set() \n",
    "\n",
    "\n",
    "# Values to filter out of CSV ranking file later\n",
    "common_words = set([\n",
    "    word for word in (set([\n",
    "        \"i\", \"and\", \"the\", \"my\", \"to\", \"a\", \"you\", \"me\", \"in\", \"so\", \"for\", \"etc\", \"by\"\n",
    "    ])) if not word.isdigit()  # Get rid of any digits (usually prices)\n",
    "])\n",
    "\n",
    "# Words to find articles if not in title\n",
    "keywords = [\n",
    "    \"dress\", \"wear\", \"look\", \"moments\", \"style\", \"celebrity\", \"celebrity style\", \"photos\", \"show\", \"shows\",\n",
    "    \"era\", \"trend\", \"wears\", \"jewelry\", \"season\", \"couture\", \"after-party\", \"girl\", \"women\", \"spring\", \"summer\",\n",
    "    \"fall\", \"winter\", \"shoe\", \"swimwear\", \"bag\", \"report\", \"week\", \"fashion week\", \"edit\", \"she\", \"lookbook\", \n",
    "    \"Fall\", \"Summer\", \"Spring\", \"Winter\", \"wardrobe\", \"outfit\", \"styles\", \"outfits\", \"idea\", \"cute\", \"wore\", \n",
    "    \"weather\", \"best\", \"fits\", \"shop\", \"Shop\", \"Jewelry\", \"jewelry\", \"Style\", \"Women\", \"Women's\", \"Fashion Week\",\"new\", \"How to\", \"cuter\", \"worth\", \"need\", \"needed\", \"these\", \"love\", \"happy\", \"guide\",\"studio\",\"brand\",\"brands\",\"clothing\"\n",
    "]\n",
    "\n",
    "# Initialize a Counter object to count word frequencies\n",
    "word_counter = Counter()\n",
    "\n",
    "\n",
    "# Initialize URL queue object to prioritize types of pages\n",
    "url_queue = PriorityQueue()\n",
    "\n",
    "\n",
    "# Function to get HTML content from a URL\n",
    "def get_html(url, retries=1, delay=35):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        elif response.status_code in [503, 429]:\n",
    "            logging.warning(f\"Received {response.status_code} status code. Retrying after delay.\")\n",
    "            time.sleep(delay)\n",
    "            if retries > 0:\n",
    "                return get_html(url, retries - 1, delay)\n",
    "            else:\n",
    "                logging.warning(f\"Retry limit exceeded for {url}. Moving on to next URL.\")\n",
    "                return None\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch HTML from {url}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred while fetching HTML from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to crawl a page and find links to scrape\n",
    "def crawl_page(url):\n",
    "    global seen_urls\n",
    "    # IF NOT SCRAPING VOGUE\n",
    "    #if url.startswith('/'):\n",
    "        #url = f\"https:/{url}\"\n",
    "\n",
    "    # VOGUE ONLY CODE IS BELOW\n",
    "    # why? well....whenever you scrape vogue, you must append to their article links the domain name. \n",
    "    if url.startswith(\"/\"):\n",
    "        url = \"https://www.vogue.com{url}\"\n",
    "    html_content = get_html(url)\n",
    "    \n",
    "    # Continuing on...\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        # Find all links on the page\n",
    "        links = [link.get('href') for link in soup.find_all('a', href=True)]\n",
    "        for link in links:\n",
    "            if re.match(r'^https?://', link) and ('page=' in link) and link not in seen_urls:  \n",
    "                url_queue.put((0, link))\n",
    "                seen_urls.add(link)\n",
    "            elif re.match(r'^/', link) and link not in seen_urls:  \n",
    "                full_url = urljoin(url, link)\n",
    "                url_queue.put((1, full_url))  \n",
    "                seen_urls.add(link)\n",
    "            elif re.match(r'^https?://', link) and any(keyword in link.lower() for keyword in keywords) and link not in seen_urls:\n",
    "                url_queue.put((2, link)) \n",
    "                seen_urls.add(link)\n",
    "\n",
    "# Get full HTML text content of the article and save it to a text file\n",
    "def extract_article_content(url, output_dir, article_title):\n",
    "    html_content = get_html(url)\n",
    "    if html_content:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        # Extract text content from article\n",
    "        text_content = \"\"\n",
    "        for tag in soup.find_all(['p', 'span', 'alt', 'title', 'h1', 'h2', 'h3']):\n",
    "            text_content += tag.get_text() + \"\\n\"\n",
    "\n",
    "        # Save text content to a file\n",
    "        file_name = article_title.replace('/', '_')  # Replace '/' in article title with '_'\n",
    "        with open(f'{output_dir}/{file_name}_content.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(text_content)\n",
    "        logging.info(f\"Article content extracted and saved to {output_dir}/{file_name}_content.txt\")      \n",
    "    else:\n",
    "        logging.error(f\"Failed to fetch HTML from {url}\")\n",
    "\n",
    "\n",
    "# Function to scrape article content from a page and its subpages\n",
    "def scrape_page(url):\n",
    "    global seen_urls\n",
    "    if url not in seen_urls:\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "        html_content = get_html(url)\n",
    "        if html_content:\n",
    "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "            title_tags = soup.find_all(['h1', 'h2', 'h3', 'a'])\n",
    "            if title_tags:\n",
    "                title = title_tags[0].get_text().lower()\n",
    "                \n",
    "                # Extract text content from article to split into individual words\n",
    "                text = \" \".join(tag.get_text() for tag in soup.find_all(['p', 'span', 'alt', 'title', 'class', 'h1', 'h2', 'h3']))\n",
    "                # Clean the text\n",
    "                cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and convert to lowercase\n",
    "                # Tokenize the text\n",
    "                words = cleaned_text.split()\n",
    "                words = [word for word in words if word not in common_words]\n",
    "                word_counter.update(words)\n",
    "                # Write article name and link to CSV to store URLS\n",
    "                article_name = title_tags[0].get_text()\n",
    "                with open('articles.csv', 'a', newline='') as csvfile:\n",
    "                    csv_writer = csv.writer(csvfile)\n",
    "                    csv_writer.writerow([url])\n",
    "\n",
    "                # Extract and save full article text for AI model training\n",
    "                output_dir = f'articletext'  \n",
    "                os.makedirs(output_dir, exist_ok=True)  \n",
    "                extract_article_content(url, output_dir, article_name)\n",
    "\n",
    "                # Extract and save images\n",
    "                images = soup.find_all('img')\n",
    "                for idx, img in enumerate(images):\n",
    "                    img_url = img.get('src')\n",
    "                    download_and_resize_image(img_url, f'{article_name}_{idx}')\n",
    "\n",
    "                logging.info(f\"Scraped page: {url}\")\n",
    "            else:\n",
    "                logging.warning(f\"No title found for page: {url}\")\n",
    "\n",
    "            # Find all links on the page and scrape subpages\n",
    "            links = [link.get('href') for link in soup.find_all(['div', 'a', 'h3', 'h6'], {'href': True})]\n",
    "            for link in links:\n",
    "                if not link.startswith('http'):\n",
    "                    # VOGUE VERSION\n",
    "                    link = urljoin('https://www.vogue.com', link) \n",
    "                    # NON-VOGUE VERSION\n",
    "                    #link = urljoin(url, link)\n",
    "                url_queue.put((1, link))\n",
    "\n",
    "            logging.info(f\"Scraped page: {url}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"Failed to scrape page: {url}\")\n",
    "\n",
    "# Function to download and resize images, accounting for different format types\n",
    "def download_and_resize_image(img_url, filename):\n",
    "    try:\n",
    "        if not img_url:\n",
    "            logging.error(\"Empty image URL provided.\")\n",
    "            return\n",
    "        parsed_url = urlparse(img_url)\n",
    "        if img_url.startswith('/'):\n",
    "            img_url = f\"{parsed_url.scheme}:{img_url}\"\n",
    "        if img_url.startswith('//'):\n",
    "            img_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{img_url}\"\n",
    "        if img_url.startswith('://'):\n",
    "            img_url = img_url[3:]\n",
    "        if img_url.startswith('data:image'):\n",
    "            img_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{img_url}\"\n",
    "        if img_url.startswith(':/'):\n",
    "            return\n",
    "\n",
    "        # Aritzia is so picky!\n",
    "        if img_url.startswith('aritzia.scene7.com'):\n",
    "            img_url = f\"https://{img_url}\" \n",
    "\n",
    "        # H&M is so picky!\n",
    "        if img_url.startswith('lp2.hm.com'):\n",
    "            img_url = f\"https://{img_url}\" \n",
    "\n",
    "\n",
    "\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            if img.mode == 'RGBA':\n",
    "                img = img.convert('RGB')\n",
    "                img.save(f'articleimages/{filename}.jpg', 'JPEG', quality=100)\n",
    "            if img.mode == 'P':\n",
    "                img = img.convert('RGB')\n",
    "                img.save(f'articleimages/{filename}.webp', 'WEBP', quality=100) \n",
    "            img = img.resize((256, 256))\n",
    "            img.save(f'articleimages/{filename}.jpg', 'JPEG', quality=95)\n",
    "        else:\n",
    "            logging.error(f\"Failed to download image from {img_url}. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error downloading or resizing image: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to process URLs from the queue\n",
    "def process_queue():\n",
    "    while True:\n",
    "        priority, url = url_queue.get()\n",
    "        if priority == 0:  # High priority \n",
    "            scrape_page(url)\n",
    "        else:  # Low priority\n",
    "            crawl_page(url)\n",
    "        url_queue.task_done()\n",
    "        time.sleep(random.uniform(3, 15))  \n",
    "\n",
    "\n",
    "# Run it!\n",
    "def main():\n",
    "\n",
    "    # Load environmental variables\n",
    "    load_dotenv()\n",
    "    ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "    SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Start URLs for crawling\n",
    "    start_urls = [\n",
    "            \"https://www.vogue.com/fashion/celebrity-style\",\n",
    "            \"https://www.vogue.com/fashion/street-style\",\n",
    "            \"https://www.vogue.com/fashion/models\",\n",
    "            \"https://www.vogue.com/fashion/trends\",\n",
    "            #\"https://www.asos.com/us/women/fashion-feed/?ctaref=ww|fashionandbeauty\",\n",
    "            #\"https://www.aritzia.com/us/en/stories\",\n",
    "            #\"https://www.aritzia.com/us/en/favourites-1\",\n",
    "    \t    #\"https://www.aritzia.com/us/en/new\",\n",
    "            #\"https://www.aritzia.com/en/clothing\",\n",
    "            #\"https://www.glamour.com/fashion\",\n",
    "            #\"https://www.cosmopolitan.com/style-beauty/fashion/\",\n",
    "            #\"https://www.elle.com/fashion/\",\n",
    "            #\"https://blog.nastygal.com/style/page/2/\",\n",
    "            #\"https://www.ssense.com/en-us/editorial/fashion\",\n",
    "            #\"https://www2.hm.com/en_us/women/seasonal-trending/trending-now.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/deals/bestsellers.html\",\n",
    "            #\"https://www.zara.com/us/en/woman-new-in-l1180.html?v1=2352540&regionGroupId=131\",\n",
    "            #\"https://www.anthropologie.com/stories-style\",\n",
    "            #\"https://www.madewell.com/Inspo.html\",\n",
    "            #\"https://www.farfetch.com/style-guide/\",\n",
    "            #\"https://www.modaoperandi.com/editorial/what-we-are-wearing\",\n",
    "            #\"https://www.brownsfashion.com/woman/stories/fashion\",\n",
    "            #\"https://www.saksfifthavenue.com/?orgin=%2Feditorial\",\n",
    "            #\"https://www.saksfifthavenue.com/c/women-s-new-arrivals\",\n",
    "            #\"https://www.ssense.com/en-us/women?sort=popularity-desc\",\n",
    "            #\"https://www.ssense.com/en-us/women\",\n",
    "            #\"https://www.abercrombie.com/shop/us/womens-new-arrivals\",\n",
    "            #\"https://shop.mango.com/us/women/featured/whats-new_d55927954?utm_source=c-producto-destacados&utm_medium=email&utm_content=woman&utm_campaign=E_WSWEOP24&sfmc_id=339434986&cjext=768854443022715810\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/trend-edit.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/tailored.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/co-ords.html\",\t\t\t\t\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/craft.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/linen.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/warm-weather.html\",\n",
    "    \t    #\"https://www2.hm.com/en_us/women/seasonal-trending/city-chic.html\",\n",
    "            #\"https://www.whowhatwear.com/section/fashion\"\n",
    "            #\"https://www.whowhatwear.com/section/style-tips\",\n",
    "            #\"https://www.whowhatwear.com/section/celebrity-style\",\n",
    "            #\"https://www.whowhatwear.com/section/outfit-ideas\",\n",
    "            #\"https://www.whowhatwear.com/section/shopping\",\n",
    "            #\"https://www.whowhatwear.com/section/trends\",\n",
    "            #\"https://www.whowhatwear.com/section/wardrobe-essentials\",\n",
    "            #\"https://www.nylon.com/fashion\",\n",
    "            #\"https://www.nylon.com/style\",\n",
    "            #\"https://www.shopcider.com/collection/new?listSource=homepage%3Bcollection_new%3B1\",\n",
    "            #\"https://www.shopcider.com/product/list?collection_id=94&link_url=https%3A%2F%2Fwww.shopcider.com%2Fproduct%2Flist%3Fcollection_id%3D94&operationpage_title=homepage&operation_position=2&operation_type=category&operation_content=Bestsellers&operation_image=&operation_update_time=1712742203550&listSource=homepage%3Bcollection_94%3B2\",\n",
    "            #\"https://www.prettylittlething.us/new-in-us.html\",\n",
    "            #\"https://www.prettylittlething.us/shop-by/trends.html\",\n",
    "            #\"https://us.princesspolly.com/collections/new\",\n",
    "            #\"https://us.princesspolly.com/collections/best-sellers\",\n",
    "            #\"https://www.aloyoga.com/collections/new-arrivals\",\n",
    "            #\"https://www.aeropostale.com/women-teen-girls/whats-new/new-arrivals/\"\n",
    "        ]\n",
    "\n",
    "    # Add start URLs to the queue\n",
    "    for url in start_urls:\n",
    "        url_queue.put((0, url))\n",
    "\n",
    "    # Create worker threads for efficiency\n",
    "    num_threads = 10\n",
    "    for _ in range(num_threads):\n",
    "        thread = threading.Thread(target=process_queue)\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "    \n",
    "    # Wait for all URLs to be scraped\n",
    "    url_queue.join()\n",
    "    print(\"-\" * 40)\n",
    "    logging.info(\"All URLs have been scraped.\")\n",
    "\n",
    "\n",
    "    # Rank word frequencies and export them to a CSV file\n",
    "    ranked_words = word_counter.most_common()\n",
    "    with open('ranked_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Word', 'Frequency'])\n",
    "        csv_writer.writerows(ranked_words)\n",
    "\n",
    "    logging.info(\"All data has been successfully exported to their respective csv. files!\")\n",
    "\n",
    "    try:\n",
    "        # Upload files to S3 bucket\n",
    "        s3 = boto3.client(\n",
    "            's3', \n",
    "            aws_access_key_id=ACCESS_KEY,\n",
    "            aws_secret_access_key=SECRET_KEY,\n",
    "        )\n",
    "        s3.upload_file('articles.csv', 'gaineddata', 'articles.csv')\n",
    "        s3.upload_file('ranked_data.csv', 'gaineddata', 'ranked_data.csv')\n",
    "        \n",
    "        for filename in os.listdir('articletext'):\n",
    "            file_path = os.path.join('articletext', filename)  \n",
    "            s3.upload_file(file_path, 'gaineddata', filename) \n",
    "\n",
    "        for filename in os.listdir('articleimages'):\n",
    "            file_path = os.path.join('articleimages', filename)\n",
    "            s3.upload_file(file_path, 'gaineddata/images/', filename)\n",
    "\n",
    "\n",
    "        logging.info(\"Files have been uploaded to S3!\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred while uploading files to S3: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# IF NOT ON VOGUE DONT APPEND, IF ON VOGUE GET CORRECT SUBPAGES, READ FUL L ARTICLES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Notes:\n",
    "# make sure to scrape on a jupyter server and not on a local machine for the AI part of this.\n",
    "# make sure when model decides new trends if old data is scraped it recognizes its already seen it and doesnt consider it as new and possibly popular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cae49e-a118-445d-a366-c69ae29d42b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
